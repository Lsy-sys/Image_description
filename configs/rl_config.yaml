# 强化学习训练配置

# 基础模型参数（继承自Transformer）
base_model:
  vocab_size: 10000
  d_model: 512
  num_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  d_ff: 2048
  dropout: 0.1
  max_len: 100

# 强化学习参数
rl:
  reward_type: 'cider_d'  # 'cider_d', 'rouge_l', 'combined'
  baseline_type: 'self_critical'  # 'self_critical', 'average'
  temperature: 1.0
  sample_size: 5  # 采样数量
  reward_weight: 1.0

# 训练参数
training:
  epochs: 20
  batch_size: 8
  learning_rate: 0.0001
  optimizer: 'adamw'
  scheduler: 'cosine'
  grad_clip: 1.0
  weight_decay: 0.01
  warmup_steps: 500

# 数据参数
data:
  max_regions: 36
  region_feature_dim: 2048
  max_caption_length: 20
  min_freq: 5
  num_workers: 4

# 路径配置
paths:
  data_dir: 'data/DeepFashion-MultiModal'
  checkpoint_dir: 'checkpoints/rl'
  log_dir: 'logs/rl'
  vocab_path: 'data/vocab.json'
  pretrained_model_path: 'checkpoints/transformer/best_model.pth'

# 其他配置
misc:
  seed: 42
  device: 'auto'
  save_every: 5
  validate_every: 1
